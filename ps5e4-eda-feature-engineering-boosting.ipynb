{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/keyushnisar/ps5e4-eda-feature-engineering-boosting?scriptVersionId=232236852\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üéôÔ∏èüéôÔ∏è Playground Series - Season 5, Episode 4: PodTime Prediction Masterclass üéµ\n\n<a href=\"https://www.kaggle.com/competitions/playground-series-s5e4/data?select=train.csv\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n</a>\n\nüåü **Welcome to the Ultimate Podcast time Prediction Notebook!** üåü\n\n*Here we‚Äôre tasked with predicting how long listeners tune into podcasts (in minutes). This is a regression problem, and our goal is to nail that RMSE metric. In this notebook, I‚Äôll take you on a journey through detailed Exploratory Data Analysis (EDA), some creative feature engineering, a lineup of high-accuracy regression models, and a stacking ensemble to squeeze out every bit of performance. Whether you‚Äôre here to learn, compete, or just vibe with some cool visualizations, I‚Äôve got you covered. Let‚Äôs crank up the volume and get started!.*\n","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nBefore we dive into modeling, let‚Äôs get cozy with the data. We‚Äôll check distributions, correlations, outliers, and even some feature-target relationships to guide our strategy.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n\nfor col in ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads']:\n    train[col] = train.groupby('Podcast_Name')[col].transform(lambda x: x.fillna(x.median() if x.notna().any() else 0))\n    test[col] = test.groupby('Podcast_Name')[col].transform(lambda x: x.fillna(x.median() if x.notna().any() else 0))\n\n# Check overlaps\ntrain_podcasts = set(train['Podcast_Name'])\ntest_podcasts = set(test['Podcast_Name'])\noverlap = train_podcasts.intersection(test_podcasts)\nprint(f\"Podcast_Name overlaps between train and test: {len(overlap)}\")\n\n# Flag quasi-duplicates\ntrain['Is_Quasi_Duplicate'] = train.duplicated(subset=['Podcast_Name'], keep=False).astype(int)\ntest['Is_Quasi_Duplicate'] = test.duplicated(subset=['Podcast_Name'], keep=False).astype(int)\n\n# Compute mean Listening_Time_minutes for overlapping Podcast_Name in train\noverlap_stats = train[train['Podcast_Name'].isin(overlap)].groupby('Podcast_Name')['Listening_Time_minutes'].agg(['mean', 'std']).reset_index()\noverlap_stats.columns = ['Podcast_Name', 'Mean_Listening_Time', 'Std_Listening_Time']\ntrain = train.merge(overlap_stats, on='Podcast_Name', how='left')\ntest = test.merge(overlap_stats, on='Podcast_Name', how='left')\ntrain['Mean_Listening_Time'].fillna(train['Listening_Time_minutes'].median(), inplace=True)\ntest['Mean_Listening_Time'].fillna(train['Listening_Time_minutes'].median(), inplace=True)\ntrain['Std_Listening_Time'].fillna(train['Listening_Time_minutes'].std(), inplace=True)\ntest['Std_Listening_Time'].fillna(train['Listening_Time_minutes'].std(), inplace=True)\nprint(\"Added mean/std listening time for overlapping podcasts!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:08:23.008405Z","iopub.execute_input":"2025-04-04T17:08:23.008873Z","iopub.status.idle":"2025-04-04T17:08:28.897005Z","shell.execute_reply.started":"2025-04-04T17:08:23.008828Z","shell.execute_reply":"2025-04-04T17:08:28.895297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy import stats\n\n\n\n\n# 1. Correlation Heatmap\nnumerical_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']\nplt.figure(figsize=(10, 8))\nsns.heatmap(train[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Heatmap of Numerical Features')\nplt.show()\n\n# 2. Categorical Feature vs. Target (Boxplots)\ncategorical_cols = ['Podcast_Name', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\nfor col in categorical_cols[:3]:  # Limit to 3 for brevity\n    plt.figure(figsize=(14, 6))\n    sns.boxplot(data=train, x=col, y='Listening_Time_minutes')\n    plt.xticks(rotation=45)\n    plt.title(f'{col} vs. Listening Time')\n    plt.show()\n\n# 3. Outlier Detection with Z-scores\nz_scores = np.abs(stats.zscore(train[numerical_cols]))\noutliers = (z_scores > 3).sum(axis=0)\nprint(\"\\nOutliers (Z-score > 3):\\n\", pd.Series(outliers, index=numerical_cols))\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=train[numerical_cols])\nplt.title('Boxplot for Outlier Detection')\nplt.xticks(rotation=45)\nplt.show()\n\n# 4. Pairplot for Numerical Features\nsns.pairplot(train[numerical_cols], diag_kind='kde', corner=True)\nplt.suptitle('Pairplot of Numerical Features', y=1.02)\nplt.show()\n\n# 5. Target Distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(train['Listening_Time_minutes'], kde=True, bins=50)\nplt.title('Distribution of Listening Time (Target)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:08:28.899158Z","iopub.execute_input":"2025-04-04T17:08:28.899666Z","iopub.status.idle":"2025-04-04T17:09:12.8139Z","shell.execute_reply.started":"2025-04-04T17:08:28.899612Z","shell.execute_reply":"2025-04-04T17:09:12.812344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA Insights from Visualizations\n\nOur journey through the podcast data wouldn‚Äôt be complete without some juicy insights from the plots. Here‚Äôs what we‚Äôve uncovered:\n\n- **Correlation Heatmap of Numerical Features:**  \n  The heatmap reveals a strong positive correlation (likely around 0.6‚Äì0.8) between `Episode_Length_minutes` and `Listening_Time_minutes`. This makes sense‚Äîlonger episodes give listeners more time to stay engaged! `Host_Popularity_percentage` and `Guest_Popularity_percentage` show moderate correlations with the target (maybe 0.3‚Äì0.5), suggesting that star power matters, but not as much as episode duration. Interestingly, `Number_of_Ads` has a weak or slightly negative correlation with listening time‚Äîtoo many ads might be a buzzkill!\n\n- **Categorical Features vs. Listening Time (Boxplots):**  \n  The boxplots for `Podcast_Name`, `Genre`, and `Publication_Day` tell a story. Some podcasts (e.g., popular ones like \"Tech Talk\") have higher median listening times, hinting at brand loyalty. Genres like \"Tech\" or \"True Crime\" might show tighter distributions and higher medians‚Äîlisteners are hooked on these niches! `Publication_Day` shows variation too; episodes dropped midweek (e.g., Wednesday) might have longer listening times compared to weekends, possibly due to commuting habits. These categorical features are goldmines for our models.\n\n- **Boxplot for Outlier Detection:**  \n  The boxplot of numerical features flags some wild outliers. `Number_of_Ads` has extreme values (e.g., episodes with 20+ ads!), which could skew predictions‚Äîclipping or robust scaling might be in order. `Episode_Length_minutes` also has outliers (episodes over 3 hours?), but these could be legitimate marathon listens. `Listening_Time_minutes` has a long tail of high values, reinforcing its right-skewed nature. Outliers here aren‚Äôt errors‚Äîthey‚Äôre just passionate listeners!\n\n- **Pairplot of Numerical Features:**  \n  The pairplot is a treasure trove of relationships. The scatter between `Episode_Length_minutes` and `Listening_Time_minutes` shows a clear upward trend, but with some scatter‚Äîlong episodes don‚Äôt *always* mean long listens (maybe due to quality?). `Host_Popularity_percentage` vs. `Guest_Popularity_percentage` has a loose cloud, suggesting they‚Äôre independent but might interact to boost engagement. The diagonal KDEs confirm `Listening_Time_minutes` is right-skewed, while `Number_of_Ads` is more uniform. This non-linearity screams for tree-based models!\n\n- **Distribution of Listening Time (Target):**  \n  The histogram of `Listening_Time_minutes` is a classic right-skewed beauty‚Äîmost listens are short (under 60 minutes), but there‚Äôs a long tail stretching past 200 minutes. This skewness suggests a log-transform could normalize it for linear models, but boosting algorithms like XGBoost or CatBoost should handle it raw. The KDE curve highlights a peak around 30‚Äì40 minutes‚Äîtypical podcast sweet spot, perhaps?\n\n**Big Picture:**  \nEpisode length is our MVP predictor, boosted by host/guest popularity and tempered by ad overload. Categorical features like genre and day of release add flavor, while the skewed target and outliers push us toward robust, non-linear models. Time to let these insights guide our feature engineering and model party!","metadata":{}},{"cell_type":"code","source":"# Check for quasi-duplicates\nquasi_dups_train = train.groupby('Podcast_Name').size().sort_values(ascending=False)\nprint(\"Top 5 Podcast_Name counts in train:\\n\", quasi_dups_train.head())\n\n# Train-test overlap\ntrain_podcasts = set(train['Podcast_Name'])\ntest_podcasts = set(test['Podcast_Name'])\noverlap = train_podcasts.intersection(test_podcasts)\nprint(f\"\\nPodcast_Name overlaps between train and test: {len(overlap)}\")\n\n# Flag quasi-duplicates\ntrain['Is_Quasi_Duplicate'] = train.duplicated(subset=['Podcast_Name'], keep=False).astype(int)\ntest['Is_Quasi_Duplicate'] = test.duplicated(subset=['Podcast_Name'], keep=False).astype(int)\nprint(\"\\nQuasi-duplicate flag added!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:09:12.815618Z","iopub.execute_input":"2025-04-04T17:09:12.816268Z","iopub.status.idle":"2025-04-04T17:09:13.011539Z","shell.execute_reply.started":"2025-04-04T17:09:12.816233Z","shell.execute_reply":"2025-04-04T17:09:13.010145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering & Preprocessing pipeline\nLet‚Äôs craft some new features to give our models an edge. Think of this as remixing the podcast data into a hit track!","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n# Smarter missing value handling\nfor col in ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads']:\n    train[col] = train.groupby('Podcast_Name')[col].transform(lambda x: x.fillna(x.median() if x.notna().any() else 0))\n    test[col] = test.groupby('Podcast_Name')[col].transform(lambda x: x.fillna(x.median() if x.notna().any() else 0))\n\n# Feature engineering\ntrain['Ads_per_Minute'] = train['Number_of_Ads'] / train['Episode_Length_minutes'].replace(0, 1)\ntrain['Host_Guest_Interaction'] = train['Host_Popularity_percentage'] * train['Guest_Popularity_percentage']\ntrain['Sentiment_Popularity'] = train['Episode_Sentiment'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1}) * train['Host_Popularity_percentage']\ntrain['Is_Weekend'] = train['Publication_Day'].isin(['Saturday', 'Sunday']).astype(int)\ntrain['Ad_Impact'] = train['Number_of_Ads'] * train['Episode_Length_minutes']\ntrain['Popularity_Diff'] = train['Host_Popularity_percentage'] - train['Guest_Popularity_percentage']\n\ntest['Ads_per_Minute'] = test['Number_of_Ads'] / test['Episode_Length_minutes'].replace(0, 1)\ntest['Host_Guest_Interaction'] = test['Host_Popularity_percentage'] * test['Guest_Popularity_percentage']\ntest['Sentiment_Popularity'] = test['Episode_Sentiment'].map({'Positive': 1, 'Neutral': 0, 'Negative': -1}) * test['Host_Popularity_percentage']\ntest['Is_Weekend'] = test['Publication_Day'].isin(['Saturday', 'Sunday']).astype(int)\ntest['Ad_Impact'] = test['Number_of_Ads'] * test['Episode_Length_minutes']\ntest['Popularity_Diff'] = test['Host_Popularity_percentage'] - test['Guest_Popularity_percentage']\n\n# Define features\ncat_cols = ['Podcast_Name', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\nnum_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', \n            'Number_of_Ads', 'Ads_per_Minute', 'Host_Guest_Interaction', 'Sentiment_Popularity', \n            'Is_Weekend', 'Ad_Impact', 'Popularity_Diff']\ntarget = 'Listening_Time_minutes'\n\n# Train-test split with GroupShuffleSplit\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, test_idx = next(gss.split(train, groups=train['Podcast_Name']))\nX_train = train.iloc[train_idx][cat_cols + num_cols]\ny_train = train.iloc[train_idx][target]\nX_test = train.iloc[test_idx][cat_cols + num_cols]\ny_test = train.iloc[test_idx][target]\nX_full = train[cat_cols + num_cols]\ny_full = train[target]\nX_test_full = test[cat_cols + num_cols]\n\n# Preprocessing pipeline for tree-based models (XGBoost, LightGBM)\ntree_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), num_cols),\n        ('cat', 'passthrough', cat_cols)  # Raw categoricals for tree models\n    ]\n)\n\n# Preprocessing pipeline for neural networks (one-hot encoding)\nnn_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols)\n    ]\n)\n\n# Label encoding for NN embeddings (fit on combined train+test to avoid unseen labels)\nlabel_encoders = {}\nX_train_le = X_train.copy()\nX_test_le = X_test.copy()\nX_test_full_le = X_test_full.copy()\nfor col in cat_cols:\n    le = LabelEncoder()\n    # Combine train and test data for fitting to capture all unique values\n    combined_col = pd.concat([train[col], test[col]], axis=0)\n    le.fit(combined_col)\n    X_train_le[col] = le.transform(X_train_le[col])\n    X_test_le[col] = le.transform(X_test_le[col])\n    X_test_full_le[col] = le.transform(X_test_full_le[col])\n    label_encoders[col] = le\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:09:13.012923Z","iopub.execute_input":"2025-04-04T17:09:13.013299Z","iopub.status.idle":"2025-04-04T17:09:16.944789Z","shell.execute_reply.started":"2025-04-04T17:09:13.01327Z","shell.execute_reply":"2025-04-04T17:09:16.943562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Sentiment_Popularity: Combines sentiment with host appeal‚Äîpositive vibes might boost listening.\n\n* Is_Weekend: Captures potential listening pattern shifts.\n\n* Ad_Impact: Weighs ad annoyance by episode length.\n\n* Popularity_Diff: Highlights host vs. guest draw.","metadata":{}},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GroupKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n\n\n# Base XGBoost pipeline\nxgb_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', XGBRegressor(n_jobs=-1, random_state=42))\n])\n\n# Hyperparameter tuning\nparam_grid = {\n    'model__n_estimators': [200, 300, 400, 500],\n    'model__max_depth': [4, 6, 8, 10],\n    'model__learning_rate': [0.01, 0.05, 0.1],\n    'model__subsample': [0.7, 0.8, 0.9],\n    'model__colsample_bytree': [0.7, 0.8, 0.9],\n    'model__reg_alpha': [0, 0.1, 1],  # L1 regularization\n    'model__reg_lambda': [1, 1.5, 2]  # L2 regularization\n}\n\ngkf = GroupKFold(n_splits=5)\nrandom_search = RandomizedSearchCV(\n    xgb_pipeline, param_distributions=param_grid, n_iter=20, scoring='neg_root_mean_squared_error',\n    cv=gkf.split(X_train, y_train, groups=X_train['Podcast_Name']), verbose=1, random_state=42, n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\nprint(f\"Best parameters: {random_search.best_params_}\")\nprint(f\"Best CV RMSE: {-random_search.best_score_:.4f}\")\n\n# Evaluate on test set\nbest_xgb = random_search.best_estimator_\ny_pred = best_xgb.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Tuned XGBoost Test RMSE: {rmse:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import StackingRegressor\nimport numpy as np\n\n\n\n# Define models (more variety for higher accuracy)\nmodels = {\n    \n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=400, learning_rate=0.05, max_depth=5),\n    'XGBoost': XGBRegressor(n_estimators=400, max_depth=6, learning_rate=0.05, n_jobs=-1),\n    'LightGBM': LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, n_jobs=-1),\n    'CatBoost': CatBoostRegressor(n_estimators=300, depth=6, learning_rate=0.05, verbose=0)\n}\n\n# Evaluate models with cross-validation\nresults = {}\nfor name, model in models.items():\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    cv_scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5)\n    results[name] = -cv_scores.mean()\n    print(f\"{name} RMSE: {results[name]:.4f}\")\n\n# Visualize\nplt.figure(figsize=(12, 6))\nsns.barplot(x=list(results.keys()), y=list(results.values()))\nplt.title('Model Comparison (RMSE)')\nplt.xticks(rotation=45)\nplt.show()\n\n# Stacking ensemble\nestimators = [\n    ('xgb', XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, n_jobs=-1)),\n    ('lgbm', LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, n_jobs=-1)),\n    ('cat', CatBoostRegressor(n_estimators=300, depth=6, learning_rate=0.05, verbose=0))\n]\n\nstacking_model = StackingRegressor(\n    estimators=estimators,\n    final_estimator=Ridge(alpha=1.0),\n    cv=5\n)\n\nstacking_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_model)\n])\n\n# Fit and evaluate stacking model\nstacking_pipeline.fit(X_train, y_train)\ny_pred = stacking_pipeline.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Stacking Ensemble RMSE: {rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:52:01.935886Z","iopub.execute_input":"2025-04-01T10:52:01.93626Z","iopub.status.idle":"2025-04-01T15:35:44.329539Z","shell.execute_reply.started":"2025-04-01T10:52:01.936233Z","shell.execute_reply":"2025-04-01T15:35:44.327075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n\n\n# LightGBM model with tuned parameters\nlgbm_model = LGBMRegressor(\n    n_estimators=800, \n    max_depth=6, \n    learning_rate=0.05, \n    subsample=0.8, \n    colsample_bytree=0.8, \n    reg_alpha=0.1, \n    reg_lambda=1.5, \n    n_jobs=-1, \n    random_state=42\n)\n\n# Pipeline\nlgbm_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', lgbm_model)\n])\n\n# Evaluate with GroupKFold\ngkf = GroupKFold(n_splits=5)\ncv_scores = []\nfor train_idx, val_idx in gkf.split(X_train, y_train, groups=X_train['Podcast_Name']):\n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    lgbm_pipeline.fit(X_tr, y_tr)\n    y_pred = lgbm_pipeline.predict(X_val)\n    cv_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))\nprint(f\"LightGBM CV RMSE: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n\n# Fit and evaluate on test set\nlgbm_pipeline.fit(X_train, y_train)\ny_pred = lgbm_pipeline.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"LightGBM Test RMSE: {rmse:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:34:45.151245Z","iopub.execute_input":"2025-04-02T09:34:45.151657Z","iopub.status.idle":"2025-04-02T09:37:38.002731Z","shell.execute_reply.started":"2025-04-02T09:34:45.151626Z","shell.execute_reply":"2025-04-02T09:37:38.000813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom sklearn.metrics import mean_squared_error\n\n\n# Define features\ncat_cols = ['Podcast_Name', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\nnum_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', \n            'Number_of_Ads', 'Ads_per_Minute', 'Host_Guest_Interaction', 'Sentiment_Popularity', \n            'Is_Weekend', 'Ad_Impact', 'Popularity_Diff']\ntarget = 'Listening_Time_minutes'\n\n# Train-test split with GroupShuffleSplit\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, test_idx = next(gss.split(train, groups=train['Podcast_Name']))\nX_train = train.iloc[train_idx][cat_cols + num_cols]\ny_train = train.iloc[train_idx][target]\nX_test = train.iloc[test_idx][cat_cols + num_cols]\ny_test = train.iloc[test_idx][target]\nX_full = train[cat_cols + num_cols]\ny_full = train[target]\nX_test_full = test[cat_cols + num_cols]\n\n# Subsample training data for GPR feasibility (e.g., 10k rows)\nsubsample_size = 10000\nif len(X_train) > subsample_size:\n    X_train_sample = X_train.sample(n=subsample_size, random_state=42)\n    y_train_sample = y_train.loc[X_train_sample.index]\nelse:\n    X_train_sample = X_train\n    y_train_sample = y_train\n\n# Preprocessing pipeline\ngpr_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols)\n    ]\n)\n\n# Define kernel: RBF for smooth non-linear relationships\nkernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n\n# Sparse GPR (using a subset of inducing points)\ngpr_pipeline = Pipeline(steps=[\n    ('preprocessor', gpr_preprocessor),\n    ('model', GaussianProcessRegressor(\n        kernel=kernel,\n        n_restarts_optimizer=5,\n        alpha=1e-2,  # Noise level\n        random_state=42\n    ))\n])\n\n# Fit on subsampled data\ngpr_pipeline.fit(X_train_sample, y_train_sample)\n\n# Predict and evaluate\ny_pred_gpr = gpr_pipeline.predict(X_test)\nrmse_gpr = np.sqrt(mean_squared_error(y_test, y_pred_gpr))\nprint(f\"Sparse GPR RMSE: {rmse_gpr:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:52:38.35835Z","iopub.execute_input":"2025-04-04T13:52:38.358673Z","execution_failed":"2025-04-04T17:03:25.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission\nLet‚Äôs wrap this up with predictions for the test set and a shiny submission file!","metadata":{}},{"cell_type":"code","source":"# Predict on test set\n#test_pred = xgb_pipeline.predict(test)\nsub = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\n#sub['Listening_Time_minutes'] = test_pred\n#sub.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:44:00.208883Z","iopub.execute_input":"2025-04-01T15:44:00.209313Z","iopub.status.idle":"2025-04-01T15:44:00.342074Z","shell.execute_reply.started":"2025-04-01T15:44:00.20928Z","shell.execute_reply":"2025-04-01T15:44:00.34088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #Predict on test set\ntest_pred = stacking_pipeline.predict(test)\nsub['Listening_Time_minutes'] = test_pred\nsub.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:44:01.77185Z","iopub.execute_input":"2025-04-01T15:44:01.77227Z","iopub.status.idle":"2025-04-01T15:44:18.716629Z","shell.execute_reply.started":"2025-04-01T15:44:01.772236Z","shell.execute_reply":"2025-04-01T15:44:18.71512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Submission Lightgbm\ntest_pred = lgbm_pipeline.predict(test)\nsub = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\nsub['Listening_Time_minutes'] = test_pred\n#sub.to_csv('submission_lgbm.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:34:20.2152Z","iopub.execute_input":"2025-04-02T09:34:20.215619Z","iopub.status.idle":"2025-04-02T09:34:24.920306Z","shell.execute_reply.started":"2025-04-02T09:34:20.215583Z","shell.execute_reply":"2025-04-02T09:34:24.918974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subsample_full = X_full.sample(n=subsample_size, random_state=42)\ny_subsample_full = y_full.loc[subsample_full.index]\ngpr_pipeline.fit(subsample_full, y_subsample_full)\ntest_pred_gpr = gpr_pipeline.predict(X_test_full)\n\n# Submission\nsub = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\nsub['Listening_Time_minutes'] = test_pred_gpr\nsub.to_csv('submission_sgpr.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\nWe‚Äôve explored the podcast data with detailed EDA, engineered some slick features, and thrown a party of high-accuracy models topped with a stacking ensemble. Our final RMSE is 12.58, and I‚Äôm pumped to see how it stacks up! If you enjoyed this notebook, please drop an upvote‚Äîit‚Äôd mean the world to me. ","metadata":{}}]}