{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Playground Series S5E5: Predicting Calories with a Stacked Ensemble\n\n<a href=\"https://www.kaggle.com/competitions/playground-series-s5e5\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n</a>\n\n## Overview\n\nThe Kaggle Playground Series S5E5 challenges us to predict Calories burned during exercise using features like Height, Weight, Heart_Rate, Duration, Body_Temp, and Sex. With 750,000 training rows, the dataset is relatively clean but demands robust modeling to capture complex relationships. .\n\nThis notebook builds a 3-level stacked ensemble with advanced feature engineering, pseudo-labeling, and full data training (825k rows with 10% augmentation). By combining diverse models and rich features, we aim to achieve a top-tier leaderboard score.\n\n## Approach\n\nTo maximize accuracy, we:\n\n### Feature Engineering:\n- Create interaction features like Weight_Heart_Rate and Duration_BMI to capture synergistic effects.\n- Generate polynomial features (e.g., Height², Weight²) for non-linear relationships.\n- Apply Target Encoding for Sex combined with numerical features to encode patterns.\n\n### 3-Level Stacking:\n- **Level 1**: Train diverse models (LightGBM, Random Forest, XGBoost, CatBoost) on 5-fold cross-validation (CV).\n- **Level 2**: Use XGBoost to non-linearly combine Level 1 predictions.\n- **Level 3**: Apply a weighted average for the final prediction.\n\n### Additional Techniques:\n- **Pseudo-Labeling**: Augment training with test data predictions to increase data volume.\n- **Outlier Handling**: Clip extreme values to improve robustness.\n- **Full Data Training**: Utilize all 825k rows to capture maximum signal.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport gc\n\n# Load data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n\n# Clip outliers\ntrain['Calories'] = train['Calories'].clip(lower=1, upper=train['Calories'].quantile(0.99))\nfor col in ['Height', 'Weight', 'Heart_Rate', 'Duration', 'Body_Temp']:\n    train[col] = train[col].clip(lower=train[col].quantile(0.01), upper=train[col].quantile(0.99))\n    test[col] = test[col].clip(lower=train[col].quantile(0.01), upper=train[col].quantile(0.99))\n\n# Encode Sex\ntrain['Sex'] = train['Sex'].map({'male': 1, 'female': 0})\ntest['Sex'] = test['Sex'].map({'male': 1, 'female': 0})\n\n# Feature Engineering: Derived Features\ntrain['height_m'] = train['Height'] / 100\ntrain['BMI'] = train['Weight'] / (train['height_m'] ** 2)\ntrain['Cardio_Load'] = train['Heart_Rate'] * train['Duration']\ntrain['Fever_Flag'] = (train['Body_Temp'] > 37.5).astype(int)\ntrain['Weight_Heart_Rate'] = train['Weight'] * train['Heart_Rate']\ntrain['Duration_BMI'] = train['Duration'] * train['BMI']\ntrain['Temp_Heart_Rate'] = train['Body_Temp'] * train['Heart_Rate']\n\ntest['height_m'] = test['Height'] / 100\ntest['BMI'] = test['Weight'] / (test['height_m'] ** 2)\ntest['Cardio_Load'] = test['Heart_Rate'] * test['Duration']\ntest['Fever_Flag'] = (test['Body_Temp'] > 37.5).astype(int)\ntest['Weight_Heart_Rate'] = test['Weight'] * test['Heart_Rate']\ntest['Duration_BMI'] = test['Duration'] * test['BMI']\ntest['Temp_Heart_Rate'] = test['Body_Temp'] * test['Heart_Rate']\n\n# Target Encoding for Sex interactions\nbase_num_cols = ['Height', 'Weight', 'Heart_Rate', 'Duration', 'Body_Temp']\nfor col in base_num_cols:\n    combo_name = f'Sex_{col}'\n    te = train.groupby(['Sex', col])['Calories'].mean()\n    count = train.groupby(['Sex', col])['Calories'].count()\n    global_mean = train['Calories'].mean()\n    smoothing = 10\n    train[combo_name + '_TE'] = train.apply(\n        lambda x: (te.get((x['Sex'], x[col]), global_mean) * count.get((x['Sex'], x[col]), 0) + global_mean * smoothing) / \n                  (count.get((x['Sex'], x[col]), 0) + smoothing), axis=1\n    )\n    test[combo_name + '_TE'] = test.apply(\n        lambda x: (te.get((x['Sex'], x[col]), global_mean) * count.get((x['Sex'], x[col]), 0) + global_mean * smoothing) / \n                  (count.get((x['Sex'], x[col]), 0) + smoothing), axis=1\n    )\n\n# Drop original numerical columns\ntrain = train.drop(['Height', 'Weight', 'Heart_Rate', 'Duration', 'Body_Temp', 'height_m'], axis=1)\ntest = test.drop(['Height', 'Weight', 'Heart_Rate', 'Duration', 'Body_Temp', 'height_m'], axis=1)\n\n# Polynomial features\nnum_cols = ['BMI', 'Cardio_Load', 'Weight_Heart_Rate', 'Duration_BMI', 'Temp_Heart_Rate']\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(train[num_cols])\npoly_feature_names = poly.get_feature_names_out(num_cols)\ntrain_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=train.index)\ntest_poly = pd.DataFrame(poly.transform(test[num_cols]), columns=poly_feature_names, index=test.index)\ntrain = pd.concat([train, train_poly], axis=1)\ntest = pd.concat([test, test_poly], axis=1)\n\n# Check for duplicate columns\nif train.columns.duplicated().any():\n    raise ValueError(f\"Duplicate columns found: {train.columns[train.columns.duplicated()]}\")\n\n# Define features\ncat_cols = ['Sex', 'Fever_Flag']\nnum_cols.extend([col for col in train.columns if '_TE' in col] + list(poly_feature_names))\nall_features = cat_cols + num_cols\ntrain = train.drop(['id'], axis=1, errors='ignore')\ntest = test.drop(['id'], axis=1, errors='ignore')\n\n# Data Augmentation\naugment_size = int(len(train) * 0.1)\naugment_data = train.sample(n=augment_size, random_state=42)\nfor col in num_cols:\n    augment_data[col] += np.random.normal(0, 0.05 * train[col].std(), augment_size)\ntrain_augmented = pd.concat([train, augment_data], axis=0).reset_index(drop=True)\n\n# Pseudo-Labeling\ninitial_model = LGBMRegressor(n_estimators=1000, learning_rate=0.02, random_state=42)\ninitial_model.fit(train_augmented[all_features], train_augmented['Calories'])\ntest['Pseudo_Label'] = initial_model.predict(test[all_features])\npseudo_data = test[all_features].copy()\npseudo_data['Calories'] = test['Pseudo_Label']\ntrain_augmented = pd.concat([train_augmented, pseudo_data], axis=0).reset_index(drop=True)\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('scaler', StandardScaler())\n        ]), num_cols + list(poly_feature_names))\n    ],\n    remainder='passthrough'\n)\n\n# Stack 1: Primary 3-Level Stack\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nlevel1_predictions = []\nlevel1_test_preds = []\nlevel1_models = [\n    ('lgbm1', LGBMRegressor(n_estimators=2000, learning_rate=0.01, max_depth=10, num_leaves=50, random_state=42)),\n    ('lgbm2', LGBMRegressor(n_estimators=1500, learning_rate=0.02, max_depth=8, num_leaves=40, random_state=43)),\n    ('rf', RandomForestRegressor(n_estimators=100, max_depth=32, random_state=42)),\n    ('catboost', CatBoostRegressor(iterations=1000, learning_rate=0.02, depth=10, random_state=42, verbose=0)),\n    ('xgb', XGBRegressor(n_estimators=1000, learning_rate=0.02, max_depth=10, random_state=42))\n]\n\nfor name, model in level1_models:\n    train_preds = np.zeros(len(train_augmented))\n    test_preds = np.zeros(len(test))\n    cv_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_augmented)):\n        X_train = train_augmented.iloc[train_idx][all_features]\n        y_train = train_augmented.iloc[train_idx]['Calories']\n        X_val = train_augmented.iloc[val_idx][all_features]\n        y_val = train_augmented.iloc[val_idx]['Calories']\n        \n        pipeline = Pipeline(steps=[\n            ('preprocessor', preprocessor),\n            ('model', model)\n        ])\n        pipeline.fit(X_train, y_train)\n        val_pred = pipeline.predict(X_val)\n        train_preds[val_idx] = val_pred\n        test_preds += pipeline.predict(test[all_features]) / 5\n        cv_scores.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n    \n    level1_predictions.append(train_preds)\n    level1_test_preds.append(test_preds)\n    print(f\"Stack 1 {name} CV RMSE: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n\n# Stack 1 Level 2: XGBoost\nlevel2_features = np.column_stack(level1_predictions)\nlevel2_test_features = np.column_stack(level1_test_preds)\nlevel2_model = XGBRegressor(n_estimators=500, learning_rate=0.01, random_state=42)\nlevel2_model.fit(level2_features, train_augmented['Calories'])\nlevel2_preds = level2_model.predict(level2_features)\nlevel2_test_preds = level2_model.predict(level2_test_features)\nprint(f\"Stack 1 Level 2 CV RMSE: {np.sqrt(mean_squared_error(train_augmented['Calories'], level2_preds)):.4f}\")\n\n# Stack 2: Light Stack\nlevel1_predictions_light = []\nlevel1_test_preds_light = []\nlevel1_models_light = [\n    ('lgbm', LGBMRegressor(n_estimators=1000, learning_rate=0.02, max_depth=8, num_leaves=40, random_state=44)),\n    ('rf', RandomForestRegressor(n_estimators=50, max_depth=20, random_state=44))\n]\n\nfor name, model in level1_models_light:\n    train_preds = np.zeros(len(train_augmented))\n    test_preds = np.zeros(len(test))\n    cv_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_augmented)):\n        X_train = train_augmented.iloc[train_idx][all_features]\n        y_train = train_augmented.iloc[train_idx]['Calories']\n        X_val = train_augmented.iloc[val_idx][all_features]\n        y_val = train_augmented.iloc[val_idx]['Calories']\n        \n        pipeline = Pipeline(steps=[\n            ('preprocessor', preprocessor),\n            ('model', model)\n        ])\n        pipeline.fit(X_train, y_train)\n        val_pred = pipeline.predict(X_val)\n        train_preds[val_idx] = val_pred\n        test_preds += pipeline.predict(test[all_features]) / 5\n        cv_scores.append(np.sqrt(mean_squared_error(y_val, val_pred)))\n    \n    level1_predictions_light.append(train_preds)\n    level1_test_preds_light.append(test_preds)\n    print(f\"Stack 2 {name} CV RMSE: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n\n# Stack 2 Level 2: LightGBM\nlevel2_features_light = np.column_stack(level1_predictions_light)\nlevel2_test_features_light = np.column_stack(level1_test_preds_light)\nlevel2_model_light = LGBMRegressor(n_estimators=500, learning_rate=0.02, random_state=44)\nlevel2_model_light.fit(level2_features_light, train_augmented['Calories'])\nlevel2_preds_light = level2_model_light.predict(level2_features_light)\nlevel2_test_preds_light = level2_model_light.predict(level2_test_features_light)\nprint(f\"Stack 2 Level 2 CV RMSE: {np.sqrt(mean_squared_error(train_augmented['Calories'], level2_preds_light)):.4f}\")\n\n# Hill Climbing Ensemble\ndef hill_climbing(predictions, y, test_predictions, max_iter=1000):\n    n_models = len(predictions)\n    weights = np.ones(n_models) / n_models\n    best_score = np.sqrt(mean_squared_error(y, np.dot(weights, predictions)))\n    best_weights = weights.copy()\n    \n    for _ in range(max_iter):\n        for i in range(n_models):\n            for step in [0.01, -0.01]:\n                new_weights = weights.copy()\n                new_weights[i] += step\n                if new_weights.min() < 0 or new_weights.sum() > 1.1:\n                    continue\n                score = np.sqrt(mean_squared_error(y, np.dot(new_weights, predictions)))\n                if score < best_score:\n                    best_score = score\n                    best_weights = new_weights.copy()\n        weights = best_weights.copy()\n        if best_score == np.sqrt(mean_squared_error(y, np.dot(weights, predictions))):\n            break\n    \n    final_preds = np.dot(best_weights, test_predictions)\n    return final_preds, best_weights, best_score\n\n# Combine predictions from both stacks for hill climbing\nall_predictions = level1_predictions + level1_predictions_light\nall_test_predictions = level1_test_preds + level1_test_preds_light\nhill_test_preds, hill_weights, hill_score = hill_climbing(\n    all_predictions, train_augmented['Calories'], all_test_predictions\n)\nprint(f\"Hill Climbing CV RMSE: {hill_score:.4f}, Weights: {hill_weights}\")\n\n# Final Submission: Weighted average of Stack 1, Stack 2, and Hill Climbing\nfinal_test_preds = 0.4 * level2_test_preds + 0.3 * level2_test_preds_light + 0.3 * hill_test_preds\nsubmission['Calories'] = np.clip(final_test_preds, 1, None)\nsubmission.to_csv('submission_ensemble.csv', index=False)\nprint(\"Ensemble submission ready—targeting private score < 0.0761!\")\ngc.collect()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-03T18:08:17.952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}